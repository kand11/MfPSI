{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrK77yj3XLvj11vYFrGFHI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kand11/MfPSI/blob/main/Task1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBhGq60WjMgV",
        "outputId": "1bc59531-5719-401a-83a8-9cf4ea3ab0ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy3\n",
            "  Downloading pymorphy3-2.0.3-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Downloading pymorphy3-2.0.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3\n",
            "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.3 pymorphy3-dicts-ru-2.4.417150.4580142\n"
          ]
        }
      ],
      "source": [
        "pip install pymorphy3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pymorphy3\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7iyovrEjOKf",
        "outputId": "ac3f3d91-ebbb-459c-9eba-5699d0b37c7c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Лемматизация происходит следующим образом:\n",
        "*   Инициализируем лемматизатор `morph`\n",
        "*   Проводим токенизацию текста `word_tokenize(text)`\n",
        "* Возвращаем результат\n",
        "\n"
      ],
      "metadata": {
        "id": "6_ejj9-uzw2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(text):\n",
        "  morph = pymorphy3.MorphAnalyzer()\n",
        "  tokens = word_tokenize(text)\n",
        "  return [morph.parse(word)[0].normal_form for word in tokens]\n",
        "\n",
        "\n",
        "text = \"Мальчики играли в футбол на улице\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\" \".join(lemmatize(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lT3d2VF6jSMw",
        "outputId": "8e61881f-ac0c-47f4-fbcf-22c00314e5e3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "мальчик играть в футбол на улица\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Стемминг происходит следующим образом:\n",
        "*   Инициализируем стеммер `stemmer`\n",
        "*   Проводим токенизацию текста `word_tokenize(text)`\n",
        "* Возвращаем результат"
      ],
      "metadata": {
        "id": "42zQhQFZ0jPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stemming(text):\n",
        "  stemmer = SnowballStemmer(\"russian\")\n",
        "  tokens = word_tokenize(text)\n",
        "  return [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "text = \"Мальчики играли в футбол на улице\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\" \".join(stemming(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFYXyF53mLfW",
        "outputId": "c4a309fb-2566-47af-9791-cd40d15b996a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "мальчик игра в футбол на улиц\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Токенизируем символы в **ASCII**:\n",
        "- Переводим числа (0..127) в `char`\n",
        "\n",
        "Векторизируем символы в **ASCII**:\n",
        "- Переводим числа (0..127) в `char`\n",
        "- Добавляем в словарь числа и соответсвующие им символы"
      ],
      "metadata": {
        "id": "mpNfq6z-0zMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def token_ascii():\n",
        "  return [chr(i) for i in range(128)]\n",
        "print(token_ascii())\n",
        "\n",
        "def vector_ascii():\n",
        "  vector = {}\n",
        "  for i in range(128):\n",
        "    vector[i] = chr(i)\n",
        "  return vector\n",
        "print(vector_ascii())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad9vu97-qQzX",
        "outputId": "3d923507-8ebf-4621-c550-13a3b20ed460"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\x00', '\\x01', '\\x02', '\\x03', '\\x04', '\\x05', '\\x06', '\\x07', '\\x08', '\\t', '\\n', '\\x0b', '\\x0c', '\\r', '\\x0e', '\\x0f', '\\x10', '\\x11', '\\x12', '\\x13', '\\x14', '\\x15', '\\x16', '\\x17', '\\x18', '\\x19', '\\x1a', '\\x1b', '\\x1c', '\\x1d', '\\x1e', '\\x1f', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x7f']\n",
            "{0: '\\x00', 1: '\\x01', 2: '\\x02', 3: '\\x03', 4: '\\x04', 5: '\\x05', 6: '\\x06', 7: '\\x07', 8: '\\x08', 9: '\\t', 10: '\\n', 11: '\\x0b', 12: '\\x0c', 13: '\\r', 14: '\\x0e', 15: '\\x0f', 16: '\\x10', 17: '\\x11', 18: '\\x12', 19: '\\x13', 20: '\\x14', 21: '\\x15', 22: '\\x16', 23: '\\x17', 24: '\\x18', 25: '\\x19', 26: '\\x1a', 27: '\\x1b', 28: '\\x1c', 29: '\\x1d', 30: '\\x1e', 31: '\\x1f', 32: ' ', 33: '!', 34: '\"', 35: '#', 36: '$', 37: '%', 38: '&', 39: \"'\", 40: '(', 41: ')', 42: '*', 43: '+', 44: ',', 45: '-', 46: '.', 47: '/', 48: '0', 49: '1', 50: '2', 51: '3', 52: '4', 53: '5', 54: '6', 55: '7', 56: '8', 57: '9', 58: ':', 59: ';', 60: '<', 61: '=', 62: '>', 63: '?', 64: '@', 65: 'A', 66: 'B', 67: 'C', 68: 'D', 69: 'E', 70: 'F', 71: 'G', 72: 'H', 73: 'I', 74: 'J', 75: 'K', 76: 'L', 77: 'M', 78: 'N', 79: 'O', 80: 'P', 81: 'Q', 82: 'R', 83: 'S', 84: 'T', 85: 'U', 86: 'V', 87: 'W', 88: 'X', 89: 'Y', 90: 'Z', 91: '[', 92: '\\\\', 93: ']', 94: '^', 95: '_', 96: '`', 97: 'a', 98: 'b', 99: 'c', 100: 'd', 101: 'e', 102: 'f', 103: 'g', 104: 'h', 105: 'i', 106: 'j', 107: 'k', 108: 'l', 109: 'm', 110: 'n', 111: 'o', 112: 'p', 113: 'q', 114: 'r', 115: 's', 116: 't', 117: 'u', 118: 'v', 119: 'w', 120: 'x', 121: 'y', 122: 'z', 123: '{', 124: '|', 125: '}', 126: '~', 127: '\\x7f'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Аналогично делаем с текстом"
      ],
      "metadata": {
        "id": "Qv4B_8k113zw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  tokens = word_tokenize(text)\n",
        "  return tokens\n",
        "\n",
        "def vectorize(text):\n",
        "  tokens = word_tokenize(text)\n",
        "  vector = {}\n",
        "  for i in range(len(tokens)):\n",
        "    vector[i+1] = tokens[i]\n",
        "  return vector\n",
        "\n",
        "\n",
        "text = \"Токенизация — это процесс разбиения текста на более мелкие части, такие как слова или предложения. Это первый шаг в анализе текста, который позволяет преобразовать непрерывный текст в дискретные элементы, с которыми можно работать отдельно. Этот процесс помогает в выявлении ключевых слов и фраз, а также в упрощении последующего анализа текста.\"\n",
        "tokenized_text = tokenize(\" \".join(lemmatize(text)))\n",
        "print(\"Токенизация после лемматизации:\" ,tokenized_text)\n",
        "tokenized_text = tokenize(\" \".join(stemming(text)))\n",
        "print(\"Токенизация после стемминга:\" ,tokenized_text)\n",
        "\n",
        "\n",
        "vectorized_text = vectorize(\" \".join(lemmatize(text)))\n",
        "print(\"Векторизация после лемматизации:\" ,vectorized_text)\n",
        "vectorized_text = vectorize(\" \".join(stemming(text)))\n",
        "print(\"Векторизация после стемминга:\" ,vectorized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GppdGUAetaaq",
        "outputId": "407269bf-303e-47fa-b50c-f616f3ec4e44"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Токенизация после лемматизации: ['токенизация', '—', 'это', 'процесс', 'разбиение', 'текст', 'на', 'более', 'мелкий', 'часть', ',', 'такой', 'как', 'слово', 'или', 'предложение', '.', 'это', 'первый', 'шаг', 'в', 'анализ', 'текст', ',', 'который', 'позволять', 'преобразовать', 'непрерывный', 'текст', 'в', 'дискретный', 'элемент', ',', 'с', 'который', 'можно', 'работать', 'отдельно', '.', 'этот', 'процесс', 'помогать', 'в', 'выявление', 'ключевой', 'слово', 'и', 'фраза', ',', 'а', 'также', 'в', 'упрощение', 'последующий', 'анализ', 'текст', '.']\n",
            "Токенизация после стемминга: ['токенизац', '—', 'эт', 'процесс', 'разбиен', 'текст', 'на', 'бол', 'мелк', 'част', ',', 'так', 'как', 'слов', 'ил', 'предложен', '.', 'эт', 'перв', 'шаг', 'в', 'анализ', 'текст', ',', 'котор', 'позволя', 'преобразова', 'непрерывн', 'текст', 'в', 'дискретн', 'элемент', ',', 'с', 'котор', 'можн', 'работа', 'отдельн', '.', 'этот', 'процесс', 'помога', 'в', 'выявлен', 'ключев', 'слов', 'и', 'фраз', ',', 'а', 'такж', 'в', 'упрощен', 'послед', 'анализ', 'текст', '.']\n",
            "Векторизация после лемматизации: {1: 'токенизация', 2: '—', 3: 'это', 4: 'процесс', 5: 'разбиение', 6: 'текст', 7: 'на', 8: 'более', 9: 'мелкий', 10: 'часть', 11: ',', 12: 'такой', 13: 'как', 14: 'слово', 15: 'или', 16: 'предложение', 17: '.', 18: 'это', 19: 'первый', 20: 'шаг', 21: 'в', 22: 'анализ', 23: 'текст', 24: ',', 25: 'который', 26: 'позволять', 27: 'преобразовать', 28: 'непрерывный', 29: 'текст', 30: 'в', 31: 'дискретный', 32: 'элемент', 33: ',', 34: 'с', 35: 'который', 36: 'можно', 37: 'работать', 38: 'отдельно', 39: '.', 40: 'этот', 41: 'процесс', 42: 'помогать', 43: 'в', 44: 'выявление', 45: 'ключевой', 46: 'слово', 47: 'и', 48: 'фраза', 49: ',', 50: 'а', 51: 'также', 52: 'в', 53: 'упрощение', 54: 'последующий', 55: 'анализ', 56: 'текст', 57: '.'}\n",
            "Векторизация после стемминга: {1: 'токенизац', 2: '—', 3: 'эт', 4: 'процесс', 5: 'разбиен', 6: 'текст', 7: 'на', 8: 'бол', 9: 'мелк', 10: 'част', 11: ',', 12: 'так', 13: 'как', 14: 'слов', 15: 'ил', 16: 'предложен', 17: '.', 18: 'эт', 19: 'перв', 20: 'шаг', 21: 'в', 22: 'анализ', 23: 'текст', 24: ',', 25: 'котор', 26: 'позволя', 27: 'преобразова', 28: 'непрерывн', 29: 'текст', 30: 'в', 31: 'дискретн', 32: 'элемент', 33: ',', 34: 'с', 35: 'котор', 36: 'можн', 37: 'работа', 38: 'отдельн', 39: '.', 40: 'этот', 41: 'процесс', 42: 'помога', 43: 'в', 44: 'выявлен', 45: 'ключев', 46: 'слов', 47: 'и', 48: 'фраз', 49: ',', 50: 'а', 51: 'такж', 52: 'в', 53: 'упрощен', 54: 'послед', 55: 'анализ', 56: 'текст', 57: '.'}\n"
          ]
        }
      ]
    }
  ]
}