{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzj0otlKxMWJaPGBCyHfww",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kand11/MfPSI/blob/main/Task1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBhGq60WjMgV",
        "outputId": "ff4bbb30-2883-45dd-fece-c4365bb0c668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy3\n",
            "  Downloading pymorphy3-2.0.3-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Downloading pymorphy3-2.0.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3\n",
            "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.3 pymorphy3-dicts-ru-2.4.417150.4580142\n"
          ]
        }
      ],
      "source": [
        "pip install pymorphy3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pymorphy3\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7iyovrEjOKf",
        "outputId": "b98cc527-0d7d-4512-ffff-f1893f44707d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Лемматизация происходит следующим образом:\n",
        "*   Инициализируем лемматизатор `morph`\n",
        "*   Проводим токенизацию текста `word_tokenize(text)`\n",
        "* Возвращаем результат\n",
        "\n"
      ],
      "metadata": {
        "id": "6_ejj9-uzw2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(text):\n",
        "  morph = pymorphy3.MorphAnalyzer()\n",
        "  tokens = word_tokenize(text)\n",
        "  return [morph.parse(word)[0].normal_form for word in tokens]\n",
        "\n",
        "\n",
        "text = \"Мальчики играли в футбол на улице\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\" \".join(lemmatize(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lT3d2VF6jSMw",
        "outputId": "cf23e0fa-2270-4958-9739-e8c5e417f21c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "мальчик играть в футбол на улица\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Стемминг происходит следующим образом:\n",
        "*   Инициализируем стеммер `stemmer`\n",
        "*   Проводим токенизацию текста `word_tokenize(text)`\n",
        "* Возвращаем результат"
      ],
      "metadata": {
        "id": "42zQhQFZ0jPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stemming(text):\n",
        "  stemmer = SnowballStemmer(\"russian\")\n",
        "  tokens = word_tokenize(text)\n",
        "  return [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "text = \"Мальчики играли в футбол на улице\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\" \".join(stemming(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFYXyF53mLfW",
        "outputId": "359311d5-8c30-4279-84c8-975b02980520"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "мальчик игра в футбол на улиц\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Токенизируем символы в **ASCII**:\n",
        "- Переводим числа (0..127) в `char`\n",
        "\n",
        "Векторизируем символы в **ASCII**:\n",
        "- Переводим числа (0..127) в `char`\n",
        "- Добавляем в словарь числа и соответсвующие им символы"
      ],
      "metadata": {
        "id": "mpNfq6z-0zMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def token_ascii():\n",
        "  return [chr(i) for i in range(128)]\n",
        "print(token_ascii())\n",
        "\n",
        "def vector_ascii():\n",
        "  vector = {}\n",
        "  for i in range(128):\n",
        "    vector[i] = chr(i)\n",
        "  return vector\n",
        "print(vector_ascii())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad9vu97-qQzX",
        "outputId": "278b0b95-39dd-4f94-c6bd-d73f7dbef531"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\x00', '\\x01', '\\x02', '\\x03', '\\x04', '\\x05', '\\x06', '\\x07', '\\x08', '\\t', '\\n', '\\x0b', '\\x0c', '\\r', '\\x0e', '\\x0f', '\\x10', '\\x11', '\\x12', '\\x13', '\\x14', '\\x15', '\\x16', '\\x17', '\\x18', '\\x19', '\\x1a', '\\x1b', '\\x1c', '\\x1d', '\\x1e', '\\x1f', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x7f']\n",
            "{0: '\\x00', 1: '\\x01', 2: '\\x02', 3: '\\x03', 4: '\\x04', 5: '\\x05', 6: '\\x06', 7: '\\x07', 8: '\\x08', 9: '\\t', 10: '\\n', 11: '\\x0b', 12: '\\x0c', 13: '\\r', 14: '\\x0e', 15: '\\x0f', 16: '\\x10', 17: '\\x11', 18: '\\x12', 19: '\\x13', 20: '\\x14', 21: '\\x15', 22: '\\x16', 23: '\\x17', 24: '\\x18', 25: '\\x19', 26: '\\x1a', 27: '\\x1b', 28: '\\x1c', 29: '\\x1d', 30: '\\x1e', 31: '\\x1f', 32: ' ', 33: '!', 34: '\"', 35: '#', 36: '$', 37: '%', 38: '&', 39: \"'\", 40: '(', 41: ')', 42: '*', 43: '+', 44: ',', 45: '-', 46: '.', 47: '/', 48: '0', 49: '1', 50: '2', 51: '3', 52: '4', 53: '5', 54: '6', 55: '7', 56: '8', 57: '9', 58: ':', 59: ';', 60: '<', 61: '=', 62: '>', 63: '?', 64: '@', 65: 'A', 66: 'B', 67: 'C', 68: 'D', 69: 'E', 70: 'F', 71: 'G', 72: 'H', 73: 'I', 74: 'J', 75: 'K', 76: 'L', 77: 'M', 78: 'N', 79: 'O', 80: 'P', 81: 'Q', 82: 'R', 83: 'S', 84: 'T', 85: 'U', 86: 'V', 87: 'W', 88: 'X', 89: 'Y', 90: 'Z', 91: '[', 92: '\\\\', 93: ']', 94: '^', 95: '_', 96: '`', 97: 'a', 98: 'b', 99: 'c', 100: 'd', 101: 'e', 102: 'f', 103: 'g', 104: 'h', 105: 'i', 106: 'j', 107: 'k', 108: 'l', 109: 'm', 110: 'n', 111: 'o', 112: 'p', 113: 'q', 114: 'r', 115: 's', 116: 't', 117: 'u', 118: 'v', 119: 'w', 120: 'x', 121: 'y', 122: 'z', 123: '{', 124: '|', 125: '}', 126: '~', 127: '\\x7f'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Аналогично делаем с текстом"
      ],
      "metadata": {
        "id": "Qv4B_8k113zw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  return [token for token in text]\n",
        "\n",
        "def vectorize(text):\n",
        "  # tokens = word_tokenize(text)\n",
        "  vector = {}\n",
        "  for token in text:\n",
        "    vector[ord(token)] = token\n",
        "  return vector\n",
        "\n",
        "\n",
        "text = \"Токенизация — это процесс разбиения текста на более мелкие части, такие как слова или предложения. Это первый шаг в анализе текста, который позволяет преобразовать непрерывный текст в дискретные элементы, с которыми можно работать отдельно. Этот процесс помогает в выявлении ключевых слов и фраз, а также в упрощении последующего анализа текста.\"\n",
        "tokenized_text = tokenize(\" \".join(lemmatize(text)))\n",
        "print(\"Токенизация после лемматизации:\" ,tokenized_text)\n",
        "tokenized_text = tokenize(\" \".join(stemming(text)))\n",
        "print(\"Токенизация после стемминга:\" ,tokenized_text)\n",
        "\n",
        "\n",
        "vectorized_text = vectorize(\" \".join(lemmatize(text)))\n",
        "print(\"Векторизация после лемматизации:\" ,vectorized_text)\n",
        "vectorized_text = vectorize(\" \".join(stemming(text)))\n",
        "print(\"Векторизация после стемминга:\" ,vectorized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GppdGUAetaaq",
        "outputId": "3d498933-90cd-4013-8301-170242f83865"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Токенизация после лемматизации: ['т', 'о', 'к', 'е', 'н', 'и', 'з', 'а', 'ц', 'и', 'я', ' ', '—', ' ', 'э', 'т', 'о', ' ', 'п', 'р', 'о', 'ц', 'е', 'с', 'с', ' ', 'р', 'а', 'з', 'б', 'и', 'е', 'н', 'и', 'е', ' ', 'т', 'е', 'к', 'с', 'т', ' ', 'н', 'а', ' ', 'б', 'о', 'л', 'е', 'е', ' ', 'м', 'е', 'л', 'к', 'и', 'й', ' ', 'ч', 'а', 'с', 'т', 'ь', ' ', ',', ' ', 'т', 'а', 'к', 'о', 'й', ' ', 'к', 'а', 'к', ' ', 'с', 'л', 'о', 'в', 'о', ' ', 'и', 'л', 'и', ' ', 'п', 'р', 'е', 'д', 'л', 'о', 'ж', 'е', 'н', 'и', 'е', ' ', '.', ' ', 'э', 'т', 'о', ' ', 'п', 'е', 'р', 'в', 'ы', 'й', ' ', 'ш', 'а', 'г', ' ', 'в', ' ', 'а', 'н', 'а', 'л', 'и', 'з', ' ', 'т', 'е', 'к', 'с', 'т', ' ', ',', ' ', 'к', 'о', 'т', 'о', 'р', 'ы', 'й', ' ', 'п', 'о', 'з', 'в', 'о', 'л', 'я', 'т', 'ь', ' ', 'п', 'р', 'е', 'о', 'б', 'р', 'а', 'з', 'о', 'в', 'а', 'т', 'ь', ' ', 'н', 'е', 'п', 'р', 'е', 'р', 'ы', 'в', 'н', 'ы', 'й', ' ', 'т', 'е', 'к', 'с', 'т', ' ', 'в', ' ', 'д', 'и', 'с', 'к', 'р', 'е', 'т', 'н', 'ы', 'й', ' ', 'э', 'л', 'е', 'м', 'е', 'н', 'т', ' ', ',', ' ', 'с', ' ', 'к', 'о', 'т', 'о', 'р', 'ы', 'й', ' ', 'м', 'о', 'ж', 'н', 'о', ' ', 'р', 'а', 'б', 'о', 'т', 'а', 'т', 'ь', ' ', 'о', 'т', 'д', 'е', 'л', 'ь', 'н', 'о', ' ', '.', ' ', 'э', 'т', 'о', 'т', ' ', 'п', 'р', 'о', 'ц', 'е', 'с', 'с', ' ', 'п', 'о', 'м', 'о', 'г', 'а', 'т', 'ь', ' ', 'в', ' ', 'в', 'ы', 'я', 'в', 'л', 'е', 'н', 'и', 'е', ' ', 'к', 'л', 'ю', 'ч', 'е', 'в', 'о', 'й', ' ', 'с', 'л', 'о', 'в', 'о', ' ', 'и', ' ', 'ф', 'р', 'а', 'з', 'а', ' ', ',', ' ', 'а', ' ', 'т', 'а', 'к', 'ж', 'е', ' ', 'в', ' ', 'у', 'п', 'р', 'о', 'щ', 'е', 'н', 'и', 'е', ' ', 'п', 'о', 'с', 'л', 'е', 'д', 'у', 'ю', 'щ', 'и', 'й', ' ', 'а', 'н', 'а', 'л', 'и', 'з', ' ', 'т', 'е', 'к', 'с', 'т', ' ', '.']\n",
            "Токенизация после стемминга: ['т', 'о', 'к', 'е', 'н', 'и', 'з', 'а', 'ц', ' ', '—', ' ', 'э', 'т', ' ', 'п', 'р', 'о', 'ц', 'е', 'с', 'с', ' ', 'р', 'а', 'з', 'б', 'и', 'е', 'н', ' ', 'т', 'е', 'к', 'с', 'т', ' ', 'н', 'а', ' ', 'б', 'о', 'л', ' ', 'м', 'е', 'л', 'к', ' ', 'ч', 'а', 'с', 'т', ' ', ',', ' ', 'т', 'а', 'к', ' ', 'к', 'а', 'к', ' ', 'с', 'л', 'о', 'в', ' ', 'и', 'л', ' ', 'п', 'р', 'е', 'д', 'л', 'о', 'ж', 'е', 'н', ' ', '.', ' ', 'э', 'т', ' ', 'п', 'е', 'р', 'в', ' ', 'ш', 'а', 'г', ' ', 'в', ' ', 'а', 'н', 'а', 'л', 'и', 'з', ' ', 'т', 'е', 'к', 'с', 'т', ' ', ',', ' ', 'к', 'о', 'т', 'о', 'р', ' ', 'п', 'о', 'з', 'в', 'о', 'л', 'я', ' ', 'п', 'р', 'е', 'о', 'б', 'р', 'а', 'з', 'о', 'в', 'а', ' ', 'н', 'е', 'п', 'р', 'е', 'р', 'ы', 'в', 'н', ' ', 'т', 'е', 'к', 'с', 'т', ' ', 'в', ' ', 'д', 'и', 'с', 'к', 'р', 'е', 'т', 'н', ' ', 'э', 'л', 'е', 'м', 'е', 'н', 'т', ' ', ',', ' ', 'с', ' ', 'к', 'о', 'т', 'о', 'р', ' ', 'м', 'о', 'ж', 'н', ' ', 'р', 'а', 'б', 'о', 'т', 'а', ' ', 'о', 'т', 'д', 'е', 'л', 'ь', 'н', ' ', '.', ' ', 'э', 'т', 'о', 'т', ' ', 'п', 'р', 'о', 'ц', 'е', 'с', 'с', ' ', 'п', 'о', 'м', 'о', 'г', 'а', ' ', 'в', ' ', 'в', 'ы', 'я', 'в', 'л', 'е', 'н', ' ', 'к', 'л', 'ю', 'ч', 'е', 'в', ' ', 'с', 'л', 'о', 'в', ' ', 'и', ' ', 'ф', 'р', 'а', 'з', ' ', ',', ' ', 'а', ' ', 'т', 'а', 'к', 'ж', ' ', 'в', ' ', 'у', 'п', 'р', 'о', 'щ', 'е', 'н', ' ', 'п', 'о', 'с', 'л', 'е', 'д', ' ', 'а', 'н', 'а', 'л', 'и', 'з', ' ', 'т', 'е', 'к', 'с', 'т', ' ', '.']\n",
            "Векторизация после лемматизации: {1090: 'т', 1086: 'о', 1082: 'к', 1077: 'е', 1085: 'н', 1080: 'и', 1079: 'з', 1072: 'а', 1094: 'ц', 1103: 'я', 32: ' ', 8212: '—', 1101: 'э', 1087: 'п', 1088: 'р', 1089: 'с', 1073: 'б', 1083: 'л', 1084: 'м', 1081: 'й', 1095: 'ч', 1100: 'ь', 44: ',', 1074: 'в', 1076: 'д', 1078: 'ж', 46: '.', 1099: 'ы', 1096: 'ш', 1075: 'г', 1102: 'ю', 1092: 'ф', 1091: 'у', 1097: 'щ'}\n",
            "Векторизация после стемминга: {1090: 'т', 1086: 'о', 1082: 'к', 1077: 'е', 1085: 'н', 1080: 'и', 1079: 'з', 1072: 'а', 1094: 'ц', 32: ' ', 8212: '—', 1101: 'э', 1087: 'п', 1088: 'р', 1089: 'с', 1073: 'б', 1083: 'л', 1084: 'м', 1095: 'ч', 44: ',', 1074: 'в', 1076: 'д', 1078: 'ж', 46: '.', 1096: 'ш', 1075: 'г', 1103: 'я', 1099: 'ы', 1100: 'ь', 1102: 'ю', 1092: 'ф', 1091: 'у', 1097: 'щ'}\n"
          ]
        }
      ]
    }
  ]
}