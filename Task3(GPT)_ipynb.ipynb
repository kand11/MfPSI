{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOs2hzXkr/Kj4AviOQLnhbv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kand11/MfPSI/blob/main/Task3(GPT)_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "03uGeh5iQmxd"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "\n",
        "def sigmoid(x):\n",
        "    x = np.clip(x, -500, 500)\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def deriv_sigmoid(x):\n",
        "    fx = sigmoid(x)\n",
        "    return fx * (1 - fx)"
      ],
      "metadata": {
        "id": "rOgoddA6Un2L"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Neuron:\n",
        "    def __init__(self, input_size):\n",
        "        self.w = np.random.randn(input_size) * 0.1\n",
        "        self.b = 0.0\n",
        "\n",
        "    def feedforward(self, x):\n",
        "        self.last_x = x\n",
        "        self.last_total = np.dot(x, self.w) + self.b\n",
        "        return sigmoid(self.last_total)\n",
        "\n",
        "    def train(self, grad_output, lr=0.01):\n",
        "        grad_total = deriv_sigmoid(self.last_total) * grad_output\n",
        "        self.w -= lr * grad_total * self.last_x\n",
        "        self.b -= lr * grad_total"
      ],
      "metadata": {
        "id": "jWoeFHnlUoap"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head:\n",
        "    def __init__(self, n_embd, head_size, block_size):\n",
        "        self.n_embd = n_embd\n",
        "        self.head_size = head_size\n",
        "        self.block_size = block_size\n",
        "\n",
        "        self.key_w = np.random.randn(n_embd, head_size) * 0.1\n",
        "        self.query_w = np.random.randn(n_embd, head_size) * 0.1\n",
        "        self.value_neuron = Neuron(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        B, T, C = x.shape\n",
        "        self.k = np.dot(x, self.key_w)\n",
        "        self.q = np.dot(x, self.query_w)\n",
        "\n",
        "        self.wei = np.matmul(self.q, self.k.transpose(0, 2, 1)) / np.sqrt(self.head_size)\n",
        "        mask = np.tril(np.ones((T, T)))\n",
        "        self.wei = np.where(mask == 1, self.wei, -1e9)\n",
        "        self.wei_softmax = softmax(self.wei)\n",
        "\n",
        "        self.v = np.zeros((B, T, 1))\n",
        "        for b in range(B):\n",
        "            for t in range(T):\n",
        "                self.v[b, t, 0] = self.value_neuron.feedforward(x[b, t])\n",
        "\n",
        "        out = np.matmul(self.wei_softmax, self.v)\n",
        "        return out\n",
        "\n",
        "    def train(self, x, y_true, lr=0.01):\n",
        "        out = self.forward(x)\n",
        "        loss = ((y_true - out) ** 2).mean()\n",
        "\n",
        "        grad_out = 2 * (out - y_true) / np.prod(y_true.shape)\n",
        "\n",
        "        for b in range(x.shape[0]):\n",
        "            for j in range(x.shape[1]):\n",
        "                grad_v = 0.0\n",
        "                for t in range(x.shape[1]):\n",
        "                    grad_v += self.wei_softmax[b, t, j] * grad_out[b, t, 0]\n",
        "                self.value_neuron.train(grad_v, lr)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "GhRh0U1HDfp2"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([\n",
        "    [[0, 0], [0, 0], [0, 0], [0, 0]],\n",
        "    [[1, 0], [0, 0], [0, 0], [0, 0]],\n",
        "    [[0, 0], [1, 0], [0, 0], [0, 0]],\n",
        "    [[0, 0], [0, 0], [0, 0], [1, 0]],\n",
        "    [[0, 0], [0, 0], [0, 0], [0, 1]],\n",
        "    [[0, 0], [1, 0], [0, 0], [0, 1]],\n",
        "    [[1, 0], [0, 1], [1, 0], [0, 1]],\n",
        "    [[0, 0], [0, 0], [0, 0], [0, 1]],\n",
        "], dtype=np.float32)\n",
        "\n",
        "Y = np.array([\n",
        "    [[0], [0], [0], [0]],\n",
        "    [[1], [1], [1], [1]],\n",
        "    [[1], [1], [1], [1]],\n",
        "    [[1], [1], [1], [1]],\n",
        "    [[1], [1], [1], [1]],\n",
        "    [[1], [1], [1], [1]],\n",
        "    [[1], [1], [1], [1]],\n",
        "    [[1], [1], [1], [1]],\n",
        "], dtype=np.float32)"
      ],
      "metadata": {
        "id": "DksB_LofUeWX"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "head = Head(n_embd=2, head_size=4, block_size=4)\n",
        "\n",
        "for epoch in range(1000):\n",
        "    loss = head.train(X, Y, lr=0.1)\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "et7rfccJU0ft",
        "outputId": "339fb34e-8c12-400c-b849-dfa93698a687"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.2422\n",
            "Epoch 100, Loss: 0.1306\n",
            "Epoch 200, Loss: 0.1177\n",
            "Epoch 300, Loss: 0.1133\n",
            "Epoch 400, Loss: 0.1111\n",
            "Epoch 500, Loss: 0.1099\n",
            "Epoch 600, Loss: 0.1092\n",
            "Epoch 700, Loss: 0.1086\n",
            "Epoch 800, Loss: 0.1083\n",
            "Epoch 900, Loss: 0.1080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_input(x_input):\n",
        "    x_input = np.array(x_input, dtype=np.float32).reshape(1, 4, 2)\n",
        "    pred = head.forward(x_input)\n",
        "    print(\"Prediction:\", np.round(pred.squeeze(), 2))\n",
        "\n",
        "test_input([[0, 0], [0, 0], [0, 0], [0, 0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpSR9DjVU_MM",
        "outputId": "d1904a37-9af9-41b0-f627-b9003a876a7a"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: [0.83 0.83 0.83 0.83]\n"
          ]
        }
      ]
    }
  ]
}