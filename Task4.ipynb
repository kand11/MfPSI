{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKg9dQMHBQCU/tUGAL97Ar",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kand11/MfPSI/blob/main/Task4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsFkQkofmgfY",
        "outputId": "45a05135-4098-4dec-a404-18a95c24ee7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy3\n",
            "  Downloading pymorphy3-2.0.3-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pymorphy3-2.0.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3\n",
            "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.3 pymorphy3-dicts-ru-2.4.417150.4580142\n"
          ]
        }
      ],
      "source": [
        "pip install pymorphy3 pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pymorphy3\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import re\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUPvY3W2mrX3",
        "outputId": "3a8bb95d-fba9-487d-8e7a-3b19bb8491d8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(text):\n",
        "  morph = pymorphy3.MorphAnalyzer()\n",
        "  tokens = word_tokenize(text)\n",
        "  return [morph.parse(word)[0].normal_form for word in tokens if not word in set(stopwords.words(\"russian\"))]\n"
      ],
      "metadata": {
        "id": "Ajj2UGbdmvNz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  tokens = word_tokenize(text)\n",
        "  return tokens\n"
      ],
      "metadata": {
        "id": "EB01JHormzXO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Подсчет для каждого текста кол-во встреч. слов\n",
        "def word_frequency_matrix(texts, unique_words):\n",
        "    word_index = {word: i for i, word in enumerate(unique_words)}\n",
        "\n",
        "    matrix = []\n",
        "    for text in texts:\n",
        "      row = [0] * len(unique_words)\n",
        "      for word in text:\n",
        "          row[word_index[word]] += 1\n",
        "      matrix.append(row)\n",
        "\n",
        "    return matrix\n"
      ],
      "metadata": {
        "id": "nnjzNiQ_-Efp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TF-IDF\n",
        "def tf_idf(texts, matrix, non_zero, unique_words):\n",
        "    word_index = {word: i for i, word in enumerate(unique_words)}\n",
        "    tf_matrix = []\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "        len_text = len(text) if len(text) > 0 else 1\n",
        "        row = [0] * len(unique_words)\n",
        "        for word in text:\n",
        "            row[word_index[word]] = (matrix[i][word_index[word]] / len_text) * non_zero [word_index[word]]\n",
        "        tf_matrix.append(row)\n",
        "\n",
        "    return tf_matrix"
      ],
      "metadata": {
        "id": "nn4D2r1H-I7j"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('train.csv')\n",
        "\n",
        "#Берем 500 строк, весь файл за 1.5 часа не обработался(проверено экспериментальным путем)\n",
        "df = df.head(500)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kx1bXHhBBJ0g",
        "outputId": "d76c1a6c-3a7c-47b9-9cd2-d4fad5ca9085"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           oid      category  \\\n",
            "0    365271984  winter_sport   \n",
            "1    503385563       extreme   \n",
            "2    146016084      football   \n",
            "3    933865449    boardgames   \n",
            "4    713550145        hockey   \n",
            "..         ...           ...   \n",
            "495  994336942    boardgames   \n",
            "496  940639075    boardgames   \n",
            "497  280447096      football   \n",
            "498  365122898     autosport   \n",
            "499  434434151        hockey   \n",
            "\n",
            "                                                  text  \n",
            "0    Волшебные фото Виктория Поплавская ЕвгенияМедв...  \n",
            "1    Возвращение в подземелье Треша 33 Эйфория тупо...  \n",
            "2    Лучшие чешские вратари – Доминик Доминатор Гаш...  \n",
            "3    Rtokenoid Warhammer40k валрак решил нас подкор...  \n",
            "4    Шестеркин затаскивает Рейнджерс в финал Восточ...  \n",
            "..                                                 ...  \n",
            "495  Майские все ближе 33 И как все знают отдыхать ...  \n",
            "496  Трехмерные поля для настольных ролевых игр от ...  \n",
            "497  Субименди откажет Барсе и подпишет новый контр...  \n",
            "498  Рекорды Формулы 1 количество сезонов до четвер...  \n",
            "499  Александр Овечкин сообщил что не будет следить...  \n",
            "\n",
            "[500 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Лемматизация\n",
        "df['lemmatized_text'] = df['text'].apply(lambda x: lemmatize(x))"
      ],
      "metadata": {
        "id": "YjLfWXw7BcVK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Токенизация\n",
        "lemm_text = [tokenize(re.sub(\"[^\\w\\s]\", \"\",\" \".join(text))) for text in df['lemmatized_text']]\n",
        "\n",
        "#Уникальные слова\n",
        "res = [item for sublist in lemm_text for item in sublist]\n",
        "unique_words = sorted(set(res))\n",
        "\n",
        "#Частотная матрица\n",
        "matrix = word_frequency_matrix(lemm_text, unique_words)\n",
        "\n",
        "#IDF\n",
        "transposed = [list(col) for col in zip(*matrix)]\n",
        "non_zero_counts = [sum(1 for num in row if num != 0) for row in transposed]\n",
        "\n",
        "#TF-IDF\n",
        "X = tf_idf(lemm_text, matrix, non_zero_counts, unique_words)"
      ],
      "metadata": {
        "id": "GqEIgc1bTCsI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Целевая переменная (категории)\n",
        "y = df['category']\n",
        "\n",
        "#Кластеризация\n",
        "kmeans = KMeans(n_clusters=df['category'].nunique(), random_state=42)\n",
        "clusters = kmeans.fit_predict(X)\n",
        "\n",
        "#Сравнение кластеров с реальными метками (было бы неплохо улучшить..)\n",
        "ari = adjusted_rand_score(y, clusters)\n",
        "print(f\"Adjusted Rand Index (сходство кластеров и реальных меток): {ari:.4f}\")\n",
        "\n",
        "#Разделение на train / val / test\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "print(f\"Train size: {len(X_train)}\")\n",
        "print(f\"Validation size: {len(X_val)}\")\n",
        "print(f\"Test size: {len(X_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3bok16iTpXM",
        "outputId": "d0a9d3d3-7fe7-4079-c04b-790ca21fd5ec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusted Rand Index (сходство кластеров и реальных меток): 0.0029\n",
            "Train size: 300\n",
            "Validation size: 100\n",
            "Test size: 100\n"
          ]
        }
      ]
    }
  ]
}